{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_data(eopatch):\n",
    "    \"\"\" filter data with cloud mask\n",
    "    \n",
    "    Returns :\n",
    "        - valid_data : times series with NaN values for cloudy pixels\n",
    "        - dates : array of dates\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of superpixel\n",
    "    n_superpixels = np.unique(eopatch.mask_timeless['SUPER_PIXELS']).size\n",
    "\n",
    "    # superpixel to which belong the corresponding pixel \n",
    "    superpixels = eopatch.mask_timeless['SUPER_PIXELS']\n",
    "\n",
    "    # number of dates\n",
    "    n_timestamps = len(eopatch.timestamp)\n",
    "\n",
    "    # temporal superpixels\n",
    "    temporal_superpixels = np.array([idx*n_superpixels + superpixels for idx in range(n_timestamps)])\n",
    "    \n",
    "    # mean of cloud coverage of each superpixel\n",
    "    mean_clm_superpixels = ndimage.mean(eopatch.mask['CLM'], labels=temporal_superpixels, index=np.unique(temporal_superpixels)).reshape((n_timestamps, n_superpixels))\n",
    "\n",
    "    # mean of ndvi of each superpixel\n",
    "    mean_ndvi_superpixels = ndimage.mean(eopatch.data['NDVI_STANDARD'], labels=temporal_superpixels, index=np.unique(temporal_superpixels)).reshape((n_timestamps, n_superpixels))\n",
    "    \n",
    "    # Apply filter : set to NaN every data that has too much cloud coverage\n",
    "    valid_data = np.where(mean_clm_superpixels<0.2, mean_ndvi_superpixels, np.nan)\n",
    "\n",
    "    # Get dates from time line\n",
    "    dates = np.array([date.strftime('%Y-%m-%d') for date in eopatch.timestamp])\n",
    "    \n",
    "    return valid_data, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest_time_series(eopatch, shapefile, filter_percentage=0.8):\n",
    "    \n",
    "    # read shapefile\n",
    "    forets = gpd.read_file(shapefile)\n",
    "    \n",
    "    # polygon of aoi\n",
    "    poly = shp.box(*eopatch.bbox)\n",
    "\n",
    "    # Projection en 4326\n",
    "    forets['geometry'] = forets['geometry'].to_crs(eopatch.bbox.crs.epsg)\n",
    "\n",
    "    # Intersection forêts et polygone superpixels\n",
    "    forets['geometry'] = forets['geometry'].intersection(poly)\n",
    "\n",
    "    # Polygones des forêts de l'aoi\n",
    "    aoi_forest = forets[~forets['geometry'].is_empty]\n",
    "\n",
    "    # dissolve polygons into a single one\n",
    "    single_poly_forest = aoi_forest.dissolve()\n",
    "    \n",
    "    # Get time series polygons\n",
    "    time_series_polygons = eopatch.vector_timeless['SUPER_PIXELS']\n",
    "\n",
    "    # Get time series polygons of region of interest (forest)\n",
    "    intersection = time_series_polygons.intersection(list(single_poly_forest.geometry)[0])\n",
    "\n",
    "    # only keep superpixel that contain more than @filter_percentage of forest\n",
    "    time_series_forest = time_series_polygons[(intersection.area / time_series_polygons.area) > filter_percentage]\n",
    "    \n",
    "    # indices of the time series preserved\n",
    "    ts_indices_preserved = np.unique(time_series_forest['VALUE']).astype('int64')\n",
    "\n",
    "    # number of time series preserved : print of the percentage\n",
    "    n_superpixels = np.unique(eopatch.mask_timeless['SUPER_PIXELS']).size\n",
    "    n_time_series_preserved = len(ts_indices_preserved)\n",
    "    print(round(len(ts_indices_preserved)/n_superpixels * 100, 2), \"% of time series preserved\")\n",
    "    \n",
    "    return time_series_forest, ts_indices_preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_town_time_series(time_series, town, filter_percentage=0.8):\n",
    "    \n",
    "    intersection = time_series.intersection(list(town)[0])\n",
    "\n",
    "    new_time_series = time_series[(intersection.area / time_series.area) > filter_percentage]\n",
    "\n",
    "    ts_indices_preserved = np.unique(new_time_series['VALUE']).astype('int64')\n",
    "    \n",
    "    return new_time_series, ts_indices_preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bfast_params(valid_data, dates, ts_indices_preserved, end_training, start_monitor, end_monitor, k=3, freq=365, trend=False, hfrac=0.25, level=0.05):\n",
    "    # change date format to datetime\n",
    "    dates = np.array([datetime.fromisoformat(str(date)) for date in dates])\n",
    "\n",
    "    # list of dates\n",
    "    dates = list(dates)\n",
    "\n",
    "    # set NaN values to 0\n",
    "    valid_data[np.isnan(valid_data)] = 0\n",
    "\n",
    "    # fit BFASTMontiro model\n",
    "    model = BFASTMonitor(\n",
    "                start_monitor,\n",
    "                freq=freq,\n",
    "                k=k,\n",
    "                hfrac=hfrac,\n",
    "                trend=trend,\n",
    "                level=level,\n",
    "                backend='python',\n",
    "                verbose=1,\n",
    "                device_id=0,\n",
    "            )\n",
    "\n",
    "    # preparing change of type\n",
    "    valid_data_int = valid_data * (32768/valid_data.max())\n",
    "\n",
    "    # change of type\n",
    "    valid_data_int = valid_data_int.astype(np.int16)\n",
    "\n",
    "    # add third dimension to make it look like an image\n",
    "    valid_data_int = valid_data_int[..., np.newaxis]\n",
    "    \n",
    "    # first date\n",
    "    start_hist = dates[0]\n",
    "\n",
    "    # crop data from start to end date of monitoring\n",
    "    valid_data_f, dates_f = crop_data_dates(valid_data_int, dates, start=start_hist - timedelta(days=1), end=end_monitor)\n",
    "\n",
    "    # filter of data\n",
    "    valid_data_f2 = valid_data_f[:, ts_indices_preserved, :]\n",
    "    \n",
    "    # dates indices\n",
    "    ind_end_train = 0\n",
    "    while dates[ind_end_train] < end_training:\n",
    "        ind_end_train+=1\n",
    "    \n",
    "    ind_start_monitor = ind_end_train\n",
    "    while dates[ind_start_monitor] < start_monitor:\n",
    "        ind_start_monitor+=1\n",
    "    \n",
    "    # filter dates\n",
    "    valid_data_f2 = np.delete(valid_data_f2, list(range(ind_end_train, ind_start_monitor)), 0)\n",
    "    del dates_f[ind_end_train:ind_start_monitor]\n",
    "    \n",
    "    return model, valid_data_f2, dates_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_bfast(model, data, dates, n_chunks=5, nan_values=0):\n",
    "    \n",
    "    old_stdout = sys.stdout # backup current stdout\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    model.fit(data, dates, n_chunks=5, nan_value=0)\n",
    "    sys.stdout = old_stdout # reset old stdout\n",
    "\n",
    "    breaks = model.breaks\n",
    "    means = model.means\n",
    "    valids = model.valids\n",
    "    magnitudes = model.magnitudes\n",
    "    \n",
    "    return breaks, magnitudes, means, valids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_results(time_series, dates, start_monitor, breaks, magnitudes):\n",
    "    # get the index of monitoring start\n",
    "    start_monitor_index = 0\n",
    "    while dates[start_monitor_index] < start_monitor:\n",
    "        start_monitor_index+=1\n",
    "\n",
    "\n",
    "    # datetime format\n",
    "    def to_date(breakpoint):\n",
    "        if breakpoint <= 0 :\n",
    "            return np.datetime64(\"NaT\")\n",
    "        bp_index = breakpoint[0]\n",
    "        return dates[start_monitor_index+bp_index]\n",
    "\n",
    "    # IF NO FOREST FILTERING \n",
    "    # superpixel dataframe\n",
    "    # super_pixels_df = eopatch.vector_timeless['SUPER_PIXELS']\n",
    "\n",
    "    # output_df = pd.DataFrame({\n",
    "    #     'VALUE':range(0,n_superpixels), \n",
    "    #     'breakpoint': [to_date(b) for b in breaks],\n",
    "    #     'magnitude': np.squeeze(magnitudes, axis=1)\n",
    "    # })\n",
    "\n",
    "    super_pixels_df = time_series\n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'VALUE': np.unique(time_series['VALUE']).astype('int64'), \n",
    "        'breakpoint': [to_date(b) for b in breaks],\n",
    "        'magnitude': np.squeeze(magnitudes, axis=1)\n",
    "    })\n",
    "\n",
    "    results = super_pixels_df.merge(output_df, on='VALUE')\n",
    "\n",
    "    # mag_norm = ((results['magnitude'] - min_mag)/(max_mag - min_mag))*2 - 1\n",
    "    results['norm_mag'] = np.zeros(len(results.index))\n",
    "    min_mag = results['magnitude'].min()\n",
    "    max_mag = results['magnitude'].max()\n",
    "\n",
    "    results['norm_mag'][results['magnitude'] > 0] = results['magnitude'][results['magnitude'] > 0]/max_mag\n",
    "    results['norm_mag'][results['magnitude'] < 0] = results['magnitude'][results['magnitude'] < 0]/abs(min_mag)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_breakpoints(results):\n",
    "    \n",
    "    # group sp by breakpoint date\n",
    "    by_breakpoints = results.groupby(by='breakpoint', as_index=False).agg({'VALUE' : 'count', 'magnitude': ['min', 'max', 'mean', 'median']})\n",
    "    by_breakpoints.columns = [f\"{x}_{y}\" if y else x for x, y in by_breakpoints.columns.to_flat_index()]\n",
    "    \n",
    "    return by_breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfast_dynamic(valid_data, dates, ts_indices_preserved, end_train, first_window):\n",
    "    \n",
    "    first_date_window = first_window[0]\n",
    "    last_date_window = first_window[1]\n",
    "    last_date_index = len(dates)-1\n",
    "\n",
    "    first_date_window_index = 0\n",
    "    while dates[first_date_window_index] < first_date_window.strftime('%Y-%m-%d'):\n",
    "        first_date_window_index+=1\n",
    "    \n",
    "    last_date_window_index = last_date_index\n",
    "    while dates[last_date_window_index] > last_date_window.strftime('%Y-%m-%d'):\n",
    "        last_date_window_index-=1\n",
    "    \n",
    "    window_dates = dates[last_date_window_index:]\n",
    "    \n",
    "    breaks_list = []\n",
    "    magnitudes_list = []\n",
    "    results = []\n",
    "    with tqdm(total=last_date_index-last_date_window_index) as pbar:\n",
    "        while last_date_window_index <= last_date_index: \n",
    "            start_monitor = datetime.fromisoformat(str(dates[first_date_window_index]))\n",
    "            end_monitor = datetime.fromisoformat(str(dates[last_date_window_index]))\n",
    "            bfast_model, valid_data_f, dates_f = set_bfast_params(valid_data, dates, ts_indices_preserved, end_train, start_monitor, end_monitor)\n",
    "            breaks, magnitudes, means, valids = execute_bfast(bfast_model, valid_data_f, dates_f);\n",
    "            #print(start_monitor, end_monitor, \"breaks =\",(breaks>0).sum())\n",
    "            breaks_list.append(breaks)\n",
    "            magnitudes_list.append(magnitudes)\n",
    "            results.append(organise_results(time_series_forest, dates_f, start_monitor, breaks, magnitudes))\n",
    "            first_date_window_index+=1\n",
    "            last_date_window_index+=1\n",
    "            #print('==================================================================')\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return breaks_list, magnitudes_list, results, window_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dep(aoi, nom_dep, basemap='OSM', shapefile=''):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    departements = gpd.read_file(shapefile)\n",
    "    departements.geometry = departements.geometry.to_crs(aoi.crs.epsg)\n",
    "    departements[departements.nom==nom_dep].iloc[[0]].plot(ax=ax, alpha=0.3, color=None, edgecolor='k', linewidth = 1)\n",
    "    gpd.GeoDataFrame(geometry=[aoi.geometry], crs=aoi.crs.pyproj_crs()).plot(ax=ax, alpha=0.3, color='red', edgecolor='r', linewidth=3)\n",
    "    if basemap=='GP':\n",
    "        cx.add_basemap(ax=ax, crs=aoi.crs.epsg, source=cx.providers.GeoportailFrance.orthos)\n",
    "    elif basemap=='OSM':\n",
    "        cx.add_basemap(ax=ax, crs=aoi.crs.epsg, source=cx.providers.OpenStreetMap.Mapnik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forest_sp(sp):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    sp.geometry.plot(ax=ax, alpha=0.5, color='red', edgecolor='k', linewidth=1)\n",
    "    cx.add_basemap(ax=ax, crs=sp.crs, source=cx.providers.GeoportailFrance.orthos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_magnitudes(results, time_series):\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(15,10))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = make_axes_locatable(ax).append_axes('right', size='2%', pad=0.1)\n",
    "    cbar = results.plot(ax=ax, column='norm_mag', cmap=cmaps.RdYlGn, legend=True, cax=cax)\n",
    "    time_series.geometry.boundary.plot(ax=ax, color=None, edgecolor='grey', linewidth=0.2)\n",
    "    cx.add_basemap(ax=ax, crs=time_series.crs, source=cx.providers.GeoportailFrance.orthos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_breakpoints(breakpoint_df, func='median'):\n",
    "    # prepare colormap\n",
    "    min_ = breakpoint_df['magnitude_'+func].min()\n",
    "    max_ = breakpoint_df['magnitude_'+func].max()\n",
    "    colormap = (breakpoint_df['magnitude_'+func] - min_)/(max_ - min_)\n",
    "\n",
    "    # plot graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sn.barplot(ax=ax, x=breakpoint_df['breakpoint'], y=breakpoint_df['VALUE_count'], palette=plt.cm.Blues_r(colormap))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    ax.set(xlabel='Dates', ylabel='Number of breakpoints')\n",
    "    #plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_high_changing_sectors(eopatch, breakpoint_df, start_date, end_date, filenames, i, path):\n",
    "    \n",
    "    palette_size = len(breakpoint_df.breakpoint)\n",
    "    red_palette = [clr.rgb2hex(plt.cm.Reds(i)) for i in range(0, plt.cm.Reds.N, round(plt.cm.Reds.N/(palette_size-1)) - 1)]\n",
    "    \n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    breakpoints = breakpoint_df.query(\"@start_date <= breakpoint <= @end_date\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    eopatch.vector_timeless['SUPER_PIXELS'].geometry.boundary.plot(ax=ax, color=None, edgecolor='black', linewidth=0.1)\n",
    "    list_bp = list(breakpoints['breakpoint'])\n",
    "    cmap = clr.ListedColormap([red_palette[b] for b in range(len(list_bp))])\n",
    "    results.query('breakpoint in @list_bp').plot(ax=ax, column='breakpoint', categorical=True, cmap=cmap, legend=True)\n",
    "    cx.add_basemap(ax=ax, crs=eopatch.bbox.crs.epsg, source=cx.providers.GeoportailFrance.orthos)\n",
    "    \n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sectors_by_bp_date(time_series_forest, results, date):\n",
    "    \n",
    "    date = date.strftime('%Y-%m-%d')\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    time_series_forest.geometry.boundary.plot(ax=ax, color=None, edgecolor='red', linewidth=0)\n",
    "    results[results.breakpoint == date].geometry.boundary.plot(ax=ax, color=None, edgecolor='red', linewidth=1)\n",
    "    cx.add_basemap(ax=ax, crs=time_series_forest.crs, source=cx.providers.GeoportailFrance.orthos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_breaks(data, date, filenames, i, path, figsize=(7,5), title='Breaks detected over time'):\n",
    "    clear_output(wait=True)\n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title + ' (' + date + ')')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(top=ceil(max_bp/1000)*1000)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of breaks detected')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_mag(data, date, filenames, i, path, figsize=(7,5), title='', ):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = make_axes_locatable(ax).append_axes('right', size='2%', pad=0.1)\n",
    "    time_series_forest.geometry.boundary.plot(ax=ax, color=None, edgecolor='grey', linewidth=0.2)\n",
    "    cx.add_basemap(ax=ax, crs=time_series_forest.crs, source=cx.providers.GeoportailFrance.orthos)\n",
    "    cbar = data.plot(ax=ax, column='norm_mag', cmap=cmaps.RdYlGn, legend=True, cax=cax)\n",
    "    \n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    plt.title('New date =' + date)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    #plt.legend(loc='center left') # the plot evolves to the right\n",
    "    \n",
    "    return filenames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
