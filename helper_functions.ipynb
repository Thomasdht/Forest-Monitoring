{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_data(eopatch):\n",
    "    \"\"\" filter time series thanks to the cloud mask\n",
    "    Inputs :\n",
    "        - eopatch : eopatch from the area of interest\n",
    "    Returns :\n",
    "        - valid_data : times series with NaN values for cloudy pixels\n",
    "        - dates : array of dates\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of superpixel\n",
    "    n_superpixels = np.unique(eopatch.mask_timeless['SUPER_PIXELS']).size\n",
    "\n",
    "    # superpixel to which belong the corresponding pixel \n",
    "    superpixels = eopatch.mask_timeless['SUPER_PIXELS']\n",
    "\n",
    "    # number of dates\n",
    "    n_timestamps = len(eopatch.timestamp)\n",
    "\n",
    "    # temporal superpixels\n",
    "    temporal_superpixels = np.array([idx*n_superpixels + superpixels for idx in range(n_timestamps)])\n",
    "    \n",
    "    # mean of cloud coverage of each superpixel\n",
    "    mean_clm_superpixels = ndimage.mean(eopatch.mask['CLM'], labels=temporal_superpixels, index=np.unique(temporal_superpixels)).reshape((n_timestamps, n_superpixels))\n",
    "\n",
    "    # mean of ndvi of each superpixel\n",
    "    mean_ndvi_superpixels = ndimage.mean(eopatch.data['NDVI_STANDARD'], labels=temporal_superpixels, index=np.unique(temporal_superpixels)).reshape((n_timestamps, n_superpixels))\n",
    "    \n",
    "    # Apply filter : set to NaN every data that has too much cloud coverage\n",
    "    valid_data = np.where(mean_clm_superpixels<0.2, mean_ndvi_superpixels, np.nan)\n",
    "\n",
    "    # Get dates from time line\n",
    "    dates = np.array([date.strftime('%Y-%m-%d') for date in eopatch.timestamp])\n",
    "    \n",
    "    return valid_data, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest_time_series(eopatch, shapefile, filter_percentage=0.8):\n",
    "    \"\"\" returns only the time series related to forest zones\n",
    "    Inputs :\n",
    "        - eopatch : eopatch from the area of interest\n",
    "        - shapefile : path to the forest shapefile\n",
    "        - filter_percentage : a time serie is considered a forest time serie if the forest coverage of the superpixel is superior to this value. 0.8 is default (e.g. 80%)\n",
    "    Returns :\n",
    "        - time_series_forest : time series that are at least 80% covered by forest\n",
    "        - ts_indices_preserved : time series indices that are preserved\n",
    "    \"\"\"\n",
    "    # read shapefile\n",
    "    forets = gpd.read_file(shapefile)\n",
    "    \n",
    "    # polygon of aoi\n",
    "    poly = shp.box(*eopatch.bbox)\n",
    "\n",
    "    # Projection en 4326\n",
    "    forets['geometry'] = forets['geometry'].to_crs(eopatch.bbox.crs.epsg)\n",
    "\n",
    "    # Intersection forêts et polygone superpixels\n",
    "    forets['geometry'] = forets['geometry'].intersection(poly)\n",
    "\n",
    "    # Polygones des forêts de l'aoi\n",
    "    aoi_forest = forets[~forets['geometry'].is_empty]\n",
    "\n",
    "    # dissolve polygons into a single one\n",
    "    single_poly_forest = aoi_forest.dissolve()\n",
    "    \n",
    "    # Get time series polygons\n",
    "    time_series_polygons = eopatch.vector_timeless['SUPER_PIXELS']\n",
    "\n",
    "    # Get time series polygons of region of interest (forest)\n",
    "    intersection = time_series_polygons.intersection(list(single_poly_forest.geometry)[0])\n",
    "\n",
    "    # only keep superpixel that contain more than @filter_percentage of forest\n",
    "    time_series_forest = time_series_polygons[(intersection.area / time_series_polygons.area) > filter_percentage]\n",
    "    \n",
    "    # indices of the time series preserved\n",
    "    ts_indices_preserved = np.unique(time_series_forest['VALUE']).astype('int64')\n",
    "\n",
    "    # number of time series preserved : print of the percentage\n",
    "    n_superpixels = np.unique(eopatch.mask_timeless['SUPER_PIXELS']).size\n",
    "    n_time_series_preserved = len(ts_indices_preserved)\n",
    "    print(round(len(ts_indices_preserved)/n_superpixels * 100, 2), \"% of time series preserved\")\n",
    "    \n",
    "    return time_series_forest, ts_indices_preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bfast_params(valid_data, dates, ts_indices_preserved, end_training, start_monitor, end_monitor, k=3, freq=365, trend=False, hfrac=0.25, level=0.05):\n",
    "    \"\"\" returns only the time series related to forest zones\n",
    "    Inputs :\n",
    "        - valid_data : the times series\n",
    "        - dates : the list of dates\n",
    "        - ts_indices_preserved : time series indices that were preserved after forest filtering\n",
    "        - end_training : date of the end of training period\n",
    "        - start_monitor : start of monitoring date\n",
    "        - end_monitor : end of monitoring date\n",
    "        - k : The number of harmonic terms. Default is 3.\n",
    "        - freq : The frequency for the seasonal model in days. Default is 365.\n",
    "        - trend : Whether a tend offset term shall be used or not. Default is False.\n",
    "        - hfrac : Float in the interval [0,1] specifying the bandwidth relative to the sample size in the MOSUM/ME monitoring processes\n",
    "        - level : Significance level of the monitoring (and ROC, if selected) procedure, i.e., probability of type I error.\n",
    "    Returns :\n",
    "        - model : the BFASTMonitor object, ready to be executed.\n",
    "        - valuid_data_f2 : the time series cropped according to the period considered\n",
    "        - dates_f : list of dates corresponding to the period considered\n",
    "    \"\"\"\n",
    "    # change date format to datetime\n",
    "    dates = np.array([datetime.fromisoformat(str(date)) for date in dates])\n",
    "\n",
    "    # list of dates\n",
    "    dates = list(dates)\n",
    "\n",
    "    # set NaN values to 0\n",
    "    valid_data[np.isnan(valid_data)] = 0\n",
    "\n",
    "    # fit BFASTMontiro model\n",
    "    model = BFASTMonitor(\n",
    "                start_monitor,\n",
    "                freq=freq,\n",
    "                k=k,\n",
    "                hfrac=hfrac,\n",
    "                trend=trend,\n",
    "                level=level,\n",
    "                backend='python',\n",
    "                verbose=1,\n",
    "                device_id=0,\n",
    "            )\n",
    "\n",
    "    # preparing change of type\n",
    "    valid_data_int = valid_data * (32768/valid_data.max())\n",
    "\n",
    "    # change of type\n",
    "    valid_data_int = valid_data_int.astype(np.int16)\n",
    "\n",
    "    # add third dimension to make it look like an image\n",
    "    valid_data_int = valid_data_int[..., np.newaxis]\n",
    "    \n",
    "    # first date\n",
    "    start_hist = dates[0]\n",
    "\n",
    "    # crop data from start to end date of monitoring\n",
    "    valid_data_f, dates_f = crop_data_dates(valid_data_int, dates, start=start_hist - timedelta(days=1), end=end_monitor)\n",
    "\n",
    "    # filter of data\n",
    "    valid_data_f2 = valid_data_f[:, ts_indices_preserved, :]\n",
    "    \n",
    "    # dates indices\n",
    "    ind_end_train = 0\n",
    "    while dates[ind_end_train] < end_training:\n",
    "        ind_end_train+=1\n",
    "    \n",
    "    ind_start_monitor = ind_end_train\n",
    "    while dates[ind_start_monitor] < start_monitor:\n",
    "        ind_start_monitor+=1\n",
    "    \n",
    "    # filter dates\n",
    "    valid_data_f2 = np.delete(valid_data_f2, list(range(ind_end_train, ind_start_monitor)), 0)\n",
    "    del dates_f[ind_end_train:ind_start_monitor]\n",
    "    \n",
    "    return model, valid_data_f2, dates_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_bfast(model, data, dates, n_chunks=5, nan_values=0):\n",
    "    \"\"\" execute bfast on time series\n",
    "    Inputs :\n",
    "        - model : the BFASTMonitor object parametered.\n",
    "        - data : time series\n",
    "        - dates : list of dates\n",
    "        - n_chunks : number of chunk to divide the job. Default is 5.\n",
    "        - nan_values : specified the NaN value used in the array data. Default is 0.\n",
    "    Returns :\n",
    "        - breaks : list of breaks that have been computed. Returns the index of the breakpoint date. -1 means no break in the time serie\n",
    "        - means : returns the mean values of the individual MOSUM processes\n",
    "        - magnitudes : values median of the difference between the data and the model prediction in the monitoring period\n",
    "        - valids : returns the number of valid values for each time series (e.g. non NaN data)\n",
    "    \"\"\"\n",
    "    old_stdout = sys.stdout # backup current stdout\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    # execute bfast\n",
    "    model.fit(data, dates, n_chunks=5, nan_value=0)\n",
    "    sys.stdout = old_stdout # reset old stdout\n",
    "\n",
    "    # return outputs\n",
    "    breaks = model.breaks\n",
    "    means = model.means\n",
    "    #valids = model.valids\n",
    "    magnitudes = model.magnitudes\n",
    "    \n",
    "    return breaks, magnitudes, means#, valids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_results(time_series, dates, start_monitor, breaks, magnitudes):\n",
    "    \"\"\" organises the results in a GeoDataFrame\n",
    "    Inputs :\n",
    "        - time_series : the time series array\n",
    "        - dates : list of dates\n",
    "        - start_monitor : start of monitoring date\n",
    "        - breaks : the breaks computed\n",
    "        - magnitudes : the magnitudes computed\n",
    "    Returns :\n",
    "        - results : GeoDataFrame tidying the results\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index of monitoring start\n",
    "    start_monitor_index = 0\n",
    "    while dates[start_monitor_index] < start_monitor:\n",
    "        start_monitor_index+=1\n",
    "\n",
    "\n",
    "    # datetime format\n",
    "    def to_date(breakpoint):\n",
    "        if breakpoint <= 0 :\n",
    "            return np.datetime64(\"NaT\")\n",
    "        bp_index = breakpoint[0]\n",
    "        return dates[start_monitor_index+bp_index]\n",
    "\n",
    "    super_pixels_df = time_series\n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'VALUE': np.unique(time_series['VALUE']).astype('int64'), \n",
    "        'breakpoint': [to_date(b) for b in breaks],\n",
    "        'magnitude': np.squeeze(magnitudes, axis=1)\n",
    "    })\n",
    "\n",
    "    results = super_pixels_df.merge(output_df, on='VALUE')\n",
    "\n",
    "    # compute normalized magnitude\n",
    "    results['norm_mag'] = np.zeros(len(results.index))\n",
    "    min_mag = results['magnitude'].min()\n",
    "    max_mag = results['magnitude'].max()\n",
    "\n",
    "    results['norm_mag'][results['magnitude'] > 0] = results['magnitude'][results['magnitude'] > 0]/max_mag\n",
    "    results['norm_mag'][results['magnitude'] < 0] = results['magnitude'][results['magnitude'] < 0]/abs(min_mag)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_breakpoints(results):\n",
    "    \"\"\" groups the breakpoints by date of appearance\n",
    "    Inputs :\n",
    "        - results : GeoDataFrame containing the results\n",
    "    Returns :\n",
    "        - group : GeoDataFrame with breakpoints grouped by date \n",
    "    \"\"\"\n",
    "    \n",
    "    # group sp by breakpoint date\n",
    "    group = results.groupby(by='breakpoint', as_index=False).agg({'VALUE' : 'count', 'magnitude': ['min', 'max', 'mean', 'median']})\n",
    "    group.columns = [f\"{x}_{y}\" if y else x for x, y in group.columns.to_flat_index()]\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfast_dynamic(valid_data, dates, ts_indices_preserved, end_train, first_window):\n",
    "    \"\"\" Applies BFAST each time the window is shifted by one date\n",
    "    Inputs :\n",
    "        - valid_data : the times series\n",
    "        - dates : the list of dates\n",
    "        - ts_indices_preserved : time series indices that were preserved after forest filtering\n",
    "        - end_training : date of the end of training period\n",
    "        - first window : first and last date of first monitoring period\n",
    "    Returns :\n",
    "        - breaks_list : list of breaks computed for each BFAST execution\n",
    "        - magnitudes_list : list of magnitudes computed for each BFAST execution\n",
    "        - results : list of results computed for each BFAST execution\n",
    "        - window_dates : list of last date of the monitoring window for each BFAST execution (needed for plotting afterwoods)\n",
    "    \"\"\"\n",
    "    \n",
    "    first_date_window = first_window[0]\n",
    "    last_date_window = first_window[1]\n",
    "    last_date_index = len(dates)-1\n",
    "\n",
    "    # find the first date index in dates\n",
    "    first_date_window_index = 0\n",
    "    while dates[first_date_window_index] < first_date_window.strftime('%Y-%m-%d'):\n",
    "        first_date_window_index+=1\n",
    "    \n",
    "    # find the last date window index in dates\n",
    "    last_date_window_index = last_date_index\n",
    "    while dates[last_date_window_index] > last_date_window.strftime('%Y-%m-%d'):\n",
    "        last_date_window_index-=1\n",
    "    \n",
    "    window_dates = dates[last_date_window_index:]\n",
    "    \n",
    "    breaks_list = []\n",
    "    magnitudes_list = []\n",
    "    results = []\n",
    "    # progress bar setting\n",
    "    with tqdm(total=last_date_index-last_date_window_index) as pbar:\n",
    "        # loop applying BFAST\n",
    "        while last_date_window_index <= last_date_index: \n",
    "            \n",
    "            # changind date format\n",
    "            start_monitor = datetime.fromisoformat(str(dates[first_date_window_index]))\n",
    "            end_monitor = datetime.fromisoformat(str(dates[last_date_window_index]))\n",
    "            \n",
    "            # set params\n",
    "            bfast_model, valid_data_f, dates_f = set_bfast_params(valid_data, dates, ts_indices_preserved, end_train, start_monitor, end_monitor)\n",
    "            \n",
    "            # execute BFAST\n",
    "            breaks, magnitudes, means = execute_bfast(bfast_model, valid_data_f, dates_f);\n",
    "            \n",
    "            # append results\n",
    "            breaks_list.append(breaks)\n",
    "            magnitudes_list.append(magnitudes)\n",
    "            results.append(organise_results(time_series_forest, dates_f, start_monitor, breaks, magnitudes))\n",
    "            \n",
    "            # increase by increment of 1 the window's edge indices (shift window to next date)\n",
    "            first_date_window_index+=1\n",
    "            last_date_window_index+=1\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return breaks_list, magnitudes_list, results, window_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dep(aoi, name, basemap='OSM', shapefile=''):\n",
    "    \"\"\" Plots the area of interests with basemap\n",
    "    Inputs :\n",
    "        - aoi : the area of interest in BBOX format\n",
    "        - name : name of the department. Lot or Var in this Notebook depending on the use case\n",
    "        - basemap : 'OSM' -> Open Street Map basemap. Default\n",
    "                    'GP' -> Géoportail basemap\n",
    "        - shapefile : French department shapefile\n",
    "    \"\"\"\n",
    "    # set figure size\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    \n",
    "    # read shapefile\n",
    "    departements = gpd.read_file(shapefile)\n",
    "    \n",
    "    # set geometry to same CRS\n",
    "    departements.geometry = departements.geometry.to_crs(aoi.crs.epsg)\n",
    "    \n",
    "    # find department shape\n",
    "    departements[departements.nom==name].iloc[[0]].plot(ax=ax, alpha=0.3, color=None, edgecolor='k', linewidth = 1)\n",
    "    \n",
    "    # plot\n",
    "    gpd.GeoDataFrame(geometry=[aoi.geometry], crs=aoi.crs.pyproj_crs()).plot(ax=ax, alpha=0.3, color='red', edgecolor='r', linewidth=3)\n",
    "    if basemap=='GP':\n",
    "        cx.add_basemap(ax=ax, crs=aoi.crs.epsg, source=cx.providers.GeoportailFrance.orthos)\n",
    "    elif basemap=='OSM':\n",
    "        cx.add_basemap(ax=ax, crs=aoi.crs.epsg, source=cx.providers.OpenStreetMap.Mapnik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forest_sp(sp):\n",
    "    \"\"\" Plots the forest superpixels\n",
    "    Inputs :\n",
    "        - sp : forest superpixels dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    sp.geometry.plot(ax=ax, alpha=0.5, color='red', edgecolor='k', linewidth=1)\n",
    "    cx.add_basemap(ax=ax, crs=sp.crs, source=cx.providers.GeoportailFrance.orthos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_magnitudes(results, time_series):\n",
    "    \"\"\" Plots the magnitudes of each superpixel\n",
    "    Inputs :\n",
    "        - results : DataFrame where the result of BFAST have been stored\n",
    "        - time_series : forest time series dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=1, figsize=(15,10))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = make_axes_locatable(ax).append_axes('right', size='2%', pad=0.1)\n",
    "    cbar = results.plot(ax=ax, column='norm_mag', cmap=cmaps.RdYlGn, legend=True, cax=cax)\n",
    "    time_series.geometry.boundary.plot(ax=ax, color=None, edgecolor='grey', linewidth=0.2)\n",
    "    cx.add_basemap(ax=ax, crs=time_series.crs, source=cx.providers.GeoportailFrance.orthos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_breakpoints(breakpoint_df, func='median'):\n",
    "    \"\"\" Plots the breakpoints per dates and displays intensity changes (bar plot)\n",
    "    Inputs :\n",
    "        - breakpoint_df : DataFrame where the breakpoints have been grouped by dates\n",
    "        - func : 'median', 'mean', 'min', or 'max'. Function applied to the magnitudes of time series detected abnormal at same date. Useful for the color gradient of the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare colormap\n",
    "    min_ = breakpoint_df['magnitude_'+func].min()\n",
    "    max_ = breakpoint_df['magnitude_'+func].max()\n",
    "    colormap = (breakpoint_df['magnitude_'+func] - min_)/(max_ - min_)\n",
    "\n",
    "    # plot graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sn.barplot(ax=ax, x=breakpoint_df['breakpoint'], y=breakpoint_df['VALUE_count'], palette=plt.cm.Blues_r(colormap))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    ax.set(xlabel='Dates', ylabel='Number of breakpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_high_changing_sectors(eopatch, breakpoint_df, start_date, end_date, filenames, i, path):\n",
    "    \"\"\" Plots high changing sectors by date of breakpoint appearance. Useful to build a GIF\n",
    "    Inputs :\n",
    "        - eopatch : eopatch of the area of interest\n",
    "        - breakpoint_df : DataFrame where the breakpoints have been grouped by dates\n",
    "        - start_date : start of monitoring date\n",
    "        - end_date : end of breakpoint consideration date\n",
    "        - filenames : list of filenames of all the plots to build the GIF\n",
    "        - i : the file number\n",
    "        - path : path where to save the plot\n",
    "    Returns : \n",
    "        - filenames : list of filenames updated\n",
    "    \"\"\"\n",
    "    \n",
    "    palette_size = len(breakpoint_df.breakpoint)\n",
    "    # build red palette\n",
    "    red_palette = [clr.rgb2hex(plt.cm.Reds(i)) for i in range(0, plt.cm.Reds.N, round(plt.cm.Reds.N/(palette_size-1)) - 1)]\n",
    "    \n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # filter all superpixels detected abnormal betweet start and end date\n",
    "    breakpoints = breakpoint_df.query(\"@start_date <= breakpoint <= @end_date\")\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    eopatch.vector_timeless['SUPER_PIXELS'].geometry.boundary.plot(ax=ax, color=None, edgecolor='black', linewidth=0.1)\n",
    "    list_bp = list(breakpoints['breakpoint'])\n",
    "    cmap = clr.ListedColormap([red_palette[b] for b in range(len(list_bp))])\n",
    "    results.query('breakpoint in @list_bp').plot(ax=ax, column='breakpoint', categorical=True, cmap=cmap, legend=True)\n",
    "    cx.add_basemap(ax=ax, crs=eopatch.bbox.crs.epsg, source=cx.providers.GeoportailFrance.orthos)\n",
    "    \n",
    "    # create name file and add it to the list\n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_breaks(data, date, filenames, i, path, max_bp, figsize=(7,5), title='Breaks detected over time'):\n",
    "    \"\"\" Plots curve of breaks detected over time. Useful to build a GIF\n",
    "    Inputs :\n",
    "        - data : list of the number of breaks detected over every sliding window\n",
    "        - date : new date added to the sliding window\n",
    "        - filenames : list of filenames of all the plots to build the GIF\n",
    "        - i : the file number\n",
    "        - path : path where to save the plot\n",
    "        - figsize : figure size. Default (7,5)\n",
    "        - title : title of the plot\n",
    "    Returns : \n",
    "        - filenames : list of filenames updated\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # create file and add it to the list\n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # build figure\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    # new date considered in sliding window\n",
    "    plt.title(title + ' (' + date + ')')\n",
    "    plt.grid(True)\n",
    "    # maximum size of y axis\n",
    "    plt.ylim(top=ceil(max_bp/1000)*1000)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of breaks detected')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_mag(data, date, filenames, i, path, figsize=(7,5), title='', ):\n",
    "    \"\"\" Plots magnitude of each superpixel. Useful to build a GIF\n",
    "    Inputs :\n",
    "        - data : list of results obtained after applying bfast_dynamic\n",
    "        - date : new date added to the sliding window\n",
    "        - filenames : list of filenames of all the plots to build the GIF\n",
    "        - i : the file number\n",
    "        - path : path where to save the plot\n",
    "        - figsize : figure size. Default (7,5)\n",
    "        - title : title of the plot\n",
    "    Returns : \n",
    "        - filenames : list of filenames updated\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = make_axes_locatable(ax).append_axes('right', size='2%', pad=0.1)\n",
    "    time_series_forest.geometry.boundary.plot(ax=ax, color=None, edgecolor='grey', linewidth=0.2)\n",
    "    cx.add_basemap(ax=ax, crs=time_series_forest.crs, source=cx.providers.GeoportailFrance.orthos)\n",
    "    cbar = data.plot(ax=ax, column='norm_mag', cmap=cmaps.RdYlGn, legend=True, cax=cax)\n",
    "    \n",
    "    # create file and add it to the list\n",
    "    filename = path + f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    plt.title('New date =' + date)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filenames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
